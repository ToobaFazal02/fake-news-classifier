"""
FAKE NEWS CLASSIFIER - TRAINING PIPELINE
Kaggle Dataset â†’ TF-IDF + LogisticRegression â†’ 95%+ Accuracy
"""

import pandas as pd
import numpy as np
import re
import joblib
import os
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import warnings
warnings.filterwarnings('ignore')

# Download NLTK data
print("ðŸ“¥ Downloading NLTK data...")
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('punkt', quiet=True)
nltk.download('omw-1.4', quiet=True)

class FakeNewsTrainer:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            ngram_range=(1, 3),
            min_df=5,
            max_df=0.8,
            sublinear_tf=True
        )
        self.model = LogisticRegression(
            C=10,
            max_iter=1000,
            class_weight='balanced',
            random_state=42,
            solver='saga',
            n_jobs=-1
        )
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
        
    def clean_text(self, text):
        """Advanced text preprocessing"""
        if pd.isna(text):
            return ""
        
        # Lowercase
        text = str(text).lower()
        
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # Remove emails
        text = re.sub(r'\S+@\S+', '', text)
        
        # Remove special characters but keep important punctuation
        text = re.sub(r'[^a-zA-Z\s!?.]', ' ', text)
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Tokenize and lemmatize
        words = text.split()
        words = [self.lemmatizer.lemmatize(word) for word in words 
                if word not in self.stop_words and len(word) > 2]
        
        return ' '.join(words)
    
    def load_kaggle_data(self):
        """Load Kaggle fake news dataset"""
        print("\nðŸ“‚ Loading Kaggle dataset...")
        
        # Try multiple Kaggle dataset formats
        try:
            # Format 1: Fake.csv + True.csv
            if os.path.exists('data/Fake.csv') and os.path.exists('data/True.csv'):
                fake = pd.read_csv('data/Fake.csv')
                real = pd.read_csv('data/True.csv')
                
                fake['label'] = 1
                real['label'] = 0
                
                # Combine text columns
                fake['text'] = fake['title'].fillna('') + ' ' + fake['text'].fillna('')
                real['text'] = real['title'].fillna('') + ' ' + real['text'].fillna('')
                
                df = pd.concat([fake[['text', 'label']], real[['text', 'label']]], ignore_index=True)
                
            # Format 2: train.csv from Kaggle competition
            elif os.path.exists('data/train.csv'):
                df = pd.read_csv('data/train.csv')
                # Assuming columns: id, title, author, text, label
                df['text'] = df['title'].fillna('') + ' ' + df['text'].fillna('')
                df = df[['text', 'label']]
                
            # Format 3: Single news.csv
            elif os.path.exists('data/news.csv'):
                df = pd.read_csv('data/news.csv')
                
            else:
                # Create sample dataset for demo
                print("âš ï¸  Kaggle dataset not found. Creating sample dataset...")
                return self.create_sample_data()
            
            # Clean up
            df = df.dropna(subset=['text', 'label'])
            df = df[df['text'].str.len() > 50]  # Remove very short articles
            
            print(f"âœ… Loaded {len(df)} articles")
            print(f"   - Fake: {sum(df['label'] == 1)}")
            print(f"   - Real: {sum(df['label'] == 0)}")
            
            return df
            
        except Exception as e:
            print(f"âŒ Error loading dataset: {e}")
            print("Creating sample dataset for demo...")
            return self.create_sample_data()
    
    def create_sample_data(self):
        """Create sample dataset for demo"""
        fake = [
            "BREAKING: Scientists discover shocking truth about vaccines that doctors don't want you to know! This miracle cure will change everything!",
            "EXPOSED: Government hiding aliens in secret base! Click here before they delete this!",
            "You won't BELIEVE what this celebrity said! Hollywood elites trying to silence the truth!",
            "SHOCKING: This one weird trick will make you rich overnight! Banks hate this!",
            "ALERT: They're tracking you through your phone! This app reveals everything they're hiding!",
        ] * 100
        
        real = [
            "The Federal Reserve announced today that interest rates will remain unchanged, citing current economic indicators and inflation data.",
            "New research published in Nature reveals promising advances in renewable energy storage technology using advanced battery systems.",
            "The United Nations climate summit concluded with member nations agreeing to enhanced emissions reduction targets for 2030.",
            "Local government officials approved the annual budget proposal following three months of public hearings and committee reviews.",
            "According to the latest employment report, unemployment rates declined slightly as job growth continued in manufacturing sectors.",
        ] * 100
        
        df = pd.DataFrame({
            'text': fake + real,
            'label': [1]*len(fake) + [0]*len(real)
        })
        
        return df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    def train(self):
        """Complete training pipeline"""
        print("\n" + "="*70)
        print("ðŸš€ FAKE NEWS CLASSIFIER - TRAINING PIPELINE")
        print("="*70)
        
        # Load data
        df = self.load_kaggle_data()
        
        # Preprocess
        print("\nðŸ§¹ Cleaning text data...")
        df['cleaned_text'] = df['text'].apply(self.clean_text)
        df = df[df['cleaned_text'].str.len() > 20]  # Remove empty after cleaning
        
        # Split data
        print("\nðŸ“Š Splitting dataset (80/20)...")
        X_train, X_test, y_train, y_test = train_test_split(
            df['cleaned_text'],
            df['label'],
            test_size=0.2,
            random_state=42,
            stratify=df['label']
        )
        
        print(f"   Training: {len(X_train)} samples")
        print(f"   Testing: {len(X_test)} samples")
        
        # Vectorize
        print("\nðŸ”¢ TF-IDF Vectorization...")
        X_train_vec = self.vectorizer.fit_transform(X_train)
        X_test_vec = self.vectorizer.transform(X_test)
        print(f"   Features: {X_train_vec.shape[1]}")
        
        # Train model
        print("\nðŸŽ“ Training Logistic Regression...")
        self.model.fit(X_train_vec, y_train)
        
        # Evaluate
        print("\nðŸ“ˆ EVALUATION RESULTS")
        print("="*70)
        
        y_pred = self.model.predict(X_test_vec)
        
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        print(f"ðŸŽ¯ Accuracy: {accuracy*100:.2f}%")
        print(f"ðŸ“Š F1-Score: {f1*100:.2f}%")
        
        print("\nðŸ“‹ Classification Report:")
        print(classification_report(y_test, y_pred, target_names=['REAL', 'FAKE']))
        
        print("\nðŸ”¢ Confusion Matrix:")
        cm = confusion_matrix(y_test, y_pred)
        print(f"   True Negatives (Realâ†’Real): {cm[0][0]}")
        print(f"   False Positives (Realâ†’Fake): {cm[0][1]}")
        print(f"   False Negatives (Fakeâ†’Real): {cm[1][0]}")
        print(f"   True Positives (Fakeâ†’Fake): {cm[1][1]}")
        
        # Save models
        print("\nðŸ’¾ Saving models...")
        os.makedirs('model', exist_ok=True)
        
        joblib.dump(self.model, 'model/classifier.pkl')
        joblib.dump(self.vectorizer, 'model/vectorizer.pkl')
        joblib.dump(self.lemmatizer, 'model/lemmatizer.pkl')
        
        # Save feature importance
        feature_names = self.vectorizer.get_feature_names_out()
        coefficients = self.model.coef_[0]
        
        # Top fake indicators
        fake_indices = coefficients.argsort()[-20:][::-1]
        real_indices = coefficients.argsort()[:20]
        
        indicators = {
            'fake_words': [feature_names[i] for i in fake_indices],
            'real_words': [feature_names[i] for i in real_indices]
        }
        joblib.dump(indicators, 'model/indicators.pkl')
        
        print("âœ… Saved:")
        print("   - model/classifier.pkl")
        print("   - model/vectorizer.pkl")
        print("   - model/lemmatizer.pkl")
        print("   - model/indicators.pkl")
        
        # Test predictions
        print("\n" + "="*70)
        print("ðŸ§ª SAMPLE PREDICTIONS")
        print("="*70)
        
        test_samples = [
            "Scientists at Harvard published peer-reviewed research on climate change impacts based on 20 years of data collection.",
            "SHOCKING! This miracle cure will change your life FOREVER! Doctors HATE this one simple trick! Click NOW before it's banned!!!"
        ]
        
        for i, sample in enumerate(test_samples, 1):
            cleaned = self.clean_text(sample)
            vec = self.vectorizer.transform([cleaned])
            pred = self.model.predict(vec)[0]
            prob = self.model.predict_proba(vec)[0]
            
            print(f"\n{i}. {sample[:80]}...")
            print(f"   Prediction: {'ðŸš¨ FAKE' if pred == 1 else 'âœ… REAL'}")
            print(f"   Confidence: {max(prob)*100:.2f}%")
        
        print("\n" + "="*70)
        print("âœ… TRAINING COMPLETE! Model ready for deployment.")
        print("="*70 + "\n")

if __name__ == "__main__":
    trainer = FakeNewsTrainer()
    trainer.train()